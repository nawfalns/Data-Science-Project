{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCMkae-6oCHT"
      },
      "outputs": [],
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports, Random Seeding"
      ],
      "metadata": {
        "id": "FF02oHhZoaVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import gc  # For garbage collection\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "W28Klde_oS9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Dataset Paths"
      ],
      "metadata": {
        "id": "7fRTTNhBomtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "cityscapes_path = \"/content/drive/MyDrive/cityscape\"\n",
        "subset_path = \"/content/drive/MyDrive/cityscapes_subset\"\n",
        "\n",
        "# Create directories for subset if they don't exist\n",
        "if not os.path.exists(subset_path):\n",
        "    os.makedirs(subset_path)\n",
        "    os.makedirs(os.path.join(subset_path, \"leftImg8bit\"))\n",
        "    os.makedirs(os.path.join(subset_path, \"gtFine\"))\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        os.makedirs(os.path.join(subset_path, \"leftImg8bit\", split))\n",
        "        os.makedirs(os.path.join(subset_path, \"gtFine\", split))\n"
      ],
      "metadata": {
        "id": "QP6TSZDZon3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subset Creation Function"
      ],
      "metadata": {
        "id": "RgXU0mePoqbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_subset(source_path, target_path, split, cities_per_split=2, images_per_city=15, target_size=(384, 192)):\n",
        "    \"\"\"\n",
        "    Create a subset of the Cityscapes dataset with reduced resolution.\n",
        "\n",
        "    Args:\n",
        "        source_path: Path to the original Cityscapes dataset\n",
        "        target_path: Path to store the subset\n",
        "        split: 'train' or 'val'\n",
        "        cities_per_split: Number of cities to include (REDUCED)\n",
        "        images_per_city: Number of images per city (REDUCED)\n",
        "        target_size: Target image size (width, height) (REDUCED)\n",
        "    \"\"\"\n",
        "    # Get list of cities for this split\n",
        "    img_cities = os.listdir(os.path.join(source_path, \"leftImg8bit\", split))\n",
        "    label_cities = os.listdir(os.path.join(source_path, \"gtFine\", split))\n",
        "\n",
        "    # Ensure the cities exist in both image and label directories\n",
        "    cities = [city for city in img_cities if city in label_cities]\n",
        "\n",
        "    # Select a subset of cities\n",
        "    if len(cities) > cities_per_split:\n",
        "        cities = sorted(cities)[:cities_per_split]\n",
        "\n",
        "    print(f\"Selected cities for {split}: {cities}\")\n",
        "\n",
        "    # Process each city\n",
        "    for city in cities:\n",
        "        # Create target directories\n",
        "        os.makedirs(os.path.join(target_path, \"leftImg8bit\", split, city), exist_ok=True)\n",
        "        os.makedirs(os.path.join(target_path, \"gtFine\", split, city), exist_ok=True)\n",
        "\n",
        "        # Get image and label files\n",
        "        img_files = sorted(os.listdir(os.path.join(source_path, \"leftImg8bit\", split, city)))\n",
        "\n",
        "        # Select a subset of images\n",
        "        if len(img_files) > images_per_city:\n",
        "            img_files = sorted(img_files)[:images_per_city]\n",
        "\n",
        "        print(f\"Processing {len(img_files)} images from {city}\")\n",
        "\n",
        "        # Process each image\n",
        "        for img_file in img_files:\n",
        "            # Extract image ID\n",
        "            img_id = img_file.split('_leftImg8bit')[0]\n",
        "\n",
        "            # Get corresponding label file\n",
        "            labelIds_file = f\"{img_id}_gtFine_labelIds.png\"\n",
        "\n",
        "            # Check if label file exists\n",
        "            if not os.path.exists(os.path.join(source_path, \"gtFine\", split, city, labelIds_file)):\n",
        "                print(f\"Warning: {labelIds_file} not found, skipping {img_file}\")\n",
        "                continue\n",
        "\n",
        "            # Load and resize image\n",
        "            img_path = os.path.join(source_path, \"leftImg8bit\", split, city, img_file)\n",
        "            img = Image.open(img_path)\n",
        "            img_resized = img.resize(target_size, Image.BILINEAR)\n",
        "\n",
        "            # Load and resize labelIds\n",
        "            label_path = os.path.join(source_path, \"gtFine\", split, city, labelIds_file)\n",
        "            label = Image.open(label_path)\n",
        "            label_resized = label.resize(target_size, Image.NEAREST)\n",
        "\n",
        "            # Save resized image and label\n",
        "            img_resized.save(os.path.join(target_path, \"leftImg8bit\", split, city, img_file))\n",
        "            label_resized.save(os.path.join(target_path, \"gtFine\", split, city, labelIds_file))\n",
        "\n",
        "print(\"Creating dataset subset...\")\n",
        "create_subset(cityscapes_path, subset_path, \"train\", cities_per_split=2, images_per_city=25)\n",
        "create_subset(cityscapes_path, subset_path, \"val\", cities_per_split=1, images_per_city=10)\n",
        "print(\"Dataset subset created!\")\n"
      ],
      "metadata": {
        "id": "55g8zZYhoyKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Cityscapes colour Mappings"
      ],
      "metadata": {
        "id": "PWK3Y8EhVDVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cityscapes ID to train ID mapping (19 evaluation classes)\n",
        "cityscapes_id_to_train_id = {\n",
        "    0: 255,  # 'unlabeled'\n",
        "    1: 255,  # 'ego vehicle'\n",
        "    2: 255,  # 'rectification border'\n",
        "    3: 255,  # 'out of roi'\n",
        "    4: 255,  # 'static'\n",
        "    5: 255,  # 'dynamic'\n",
        "    6: 255,  # 'ground'\n",
        "    7: 0,    # 'road'\n",
        "    8: 1,    # 'sidewalk'\n",
        "    9: 255,  # 'parking'\n",
        "    10: 255, # 'rail track'\n",
        "    11: 2,   # 'building'\n",
        "    12: 3,   # 'wall'\n",
        "    13: 4,   # 'fence'\n",
        "    14: 255, # 'guard rail'\n",
        "    15: 255, # 'bridge'\n",
        "    16: 255, # 'tunnel'\n",
        "    17: 5,   # 'pole'\n",
        "    18: 255, # 'polegroup'\n",
        "    19: 6,   # 'traffic light'\n",
        "    20: 7,   # 'traffic sign'\n",
        "    21: 8,   # 'vegetation'\n",
        "    22: 9,   # 'terrain'\n",
        "    23: 10,  # 'sky'\n",
        "    24: 11,  # 'person'\n",
        "    25: 12,  # 'rider'\n",
        "    26: 13,  # 'car'\n",
        "    27: 14,  # 'truck'\n",
        "    28: 15,  # 'bus'\n",
        "    29: 255, # 'caravan'\n",
        "    30: 255, # 'trailer'\n",
        "    31: 16,  # 'train'\n",
        "    32: 17,  # 'motorcycle'\n",
        "    33: 18,  # 'bicycle'\n",
        "    -1: 255, # 'license plate'\n",
        "}\n",
        "\n",
        "# Define class names for visualization\n",
        "cityscapes_classes = {\n",
        "    0: 'road', 1: 'sidewalk', 2: 'building', 3: 'wall', 4: 'fence',\n",
        "    5: 'pole', 6: 'traffic light', 7: 'traffic sign', 8: 'vegetation',\n",
        "    9: 'terrain', 10: 'sky', 11: 'person', 12: 'rider', 13: 'car',\n",
        "    14: 'truck', 15: 'bus', 16: 'train', 17: 'motorcycle', 18: 'bicycle'\n",
        "}\n"
      ],
      "metadata": {
        "id": "MQ7wX8ZYVEI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Define Custom Dataset and Transform"
      ],
      "metadata": {
        "id": "dFYK4RPKVJkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# OPTIMIZATION: Improved custom dataset class for Cityscapes\n",
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None, cache_data=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: Root directory of the Cityscapes dataset\n",
        "            split: 'train' or 'val'\n",
        "            transform: Optional transform to be applied on a sample\n",
        "            cache_data: Whether to cache dataset in memory\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.cache_data = cache_data\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.data_cache = {} if cache_data else None\n",
        "\n",
        "        # Get list of all images and labels\n",
        "        img_dir = os.path.join(root_dir, \"leftImg8bit\", split)\n",
        "        label_dir = os.path.join(root_dir, \"gtFine\", split)\n",
        "\n",
        "        for city in os.listdir(img_dir):\n",
        "            city_img_dir = os.path.join(img_dir, city)\n",
        "            city_label_dir = os.path.join(label_dir, city)\n",
        "\n",
        "            for file_name in os.listdir(city_img_dir):\n",
        "                if file_name.endswith(\"_leftImg8bit.png\"):\n",
        "                    img_path = os.path.join(city_img_dir, file_name)\n",
        "\n",
        "                    # Extract image ID\n",
        "                    img_id = file_name.split('_leftImg8bit')[0]\n",
        "\n",
        "                    # Get corresponding label file\n",
        "                    label_file = f\"{img_id}_gtFine_labelIds.png\"\n",
        "                    label_path = os.path.join(city_label_dir, label_file)\n",
        "\n",
        "                    if os.path.exists(label_path):\n",
        "                        self.images.append(img_path)\n",
        "                        self.labels.append(label_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Check if item is in cache\n",
        "        if self.cache_data and idx in self.data_cache:\n",
        "            return self.data_cache[idx]\n",
        "\n",
        "        img_path = self.images[idx]\n",
        "        label_path = self.labels[idx]\n",
        "\n",
        "        # Load image and label\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = Image.open(label_path)\n",
        "\n",
        "        # Convert label IDs to train IDs efficiently\n",
        "        label_np = np.array(label)\n",
        "        label_train_id = np.ones_like(label_np) * 255  # Initialize with ignore index\n",
        "\n",
        "        for id, train_id in cityscapes_id_to_train_id.items():\n",
        "            label_train_id[label_np == id] = train_id\n",
        "\n",
        "        label = Image.fromarray(label_train_id.astype(np.uint8))\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image, label = self.transform(image, label)\n",
        "\n",
        "        # Cache item if enabled\n",
        "        if self.cache_data:\n",
        "            self.data_cache[idx] = (image, label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# OPTIMIZATION: Simplified data augmentation with fewer transformations\n",
        "class CityscapesTransform:\n",
        "    def __init__(self, split='train', img_size=(384, 192)):\n",
        "        self.split = split\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __call__(self, image, label):\n",
        "        # Ensure consistent image size for both image and label\n",
        "        image = image.resize(self.img_size, Image.BILINEAR)\n",
        "        label = label.resize(self.img_size, Image.NEAREST)\n",
        "\n",
        "        # Apply minimal data augmentation for training to save computation\n",
        "        if self.split == 'train':\n",
        "            # Random horizontal flip only\n",
        "            if random.random() > 0.5:\n",
        "                image = TF.hflip(image)\n",
        "                label = TF.hflip(label)\n",
        "\n",
        "        # Convert to tensor\n",
        "        image = TF.to_tensor(image)\n",
        "        label = torch.from_numpy(np.array(label)).long()\n",
        "\n",
        "        # Normalize image\n",
        "        image = TF.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "RwEsSxXRVKNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Architecture, Metrics"
      ],
      "metadata": {
        "id": "AV1j9PP3VTqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIMIZATION: Lightweight ResNet block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# OPTIMIZATION: Lightweight model architecture\n",
        "class LightweightSegmentation(nn.Module):\n",
        "    def __init__(self, block, num_classes=19):\n",
        "        super(LightweightSegmentation, self).__init__()\n",
        "\n",
        "        # OPTIMIZATION: Reduced number of filters throughout the network\n",
        "        self.in_channels = 32  # Reduced from 64\n",
        "\n",
        "        # Initial convolution with stride 2 to reduce spatial dimensions\n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # ResNet layers with reduced layers\n",
        "        self.layer1 = self._make_layer(block, 32, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(block, 64, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(block, 128, 2, stride=2)\n",
        "\n",
        "        # OPTIMIZATION: Skip layer4 to reduce parameters and computation\n",
        "\n",
        "        # OPTIMIZATION: Simplified decoder path\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.upbn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
        "        self.upbn2 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1)\n",
        "        self.upbn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.final_conv = nn.Conv2d(32, num_classes, kernel_size=3, padding=1)\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Save input size for potential interpolation\n",
        "        input_size = (x.size(2), x.size(3))\n",
        "\n",
        "        # Encoder path\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        # Decoder path\n",
        "        x = self.upconv1(x)\n",
        "        x = self.upbn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.upconv2(x)\n",
        "        x = self.upbn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.upconv3(x)\n",
        "        x = self.upbn3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Final classification\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # Ensure output matches input size through interpolation if needed\n",
        "        if x.size(2) != input_size[0] or x.size(3) != input_size[1]:\n",
        "            x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=True)\n",
        "\n",
        "        return x\n",
        "\n",
        "# OPTIMIZATION: Create lightweight model\n",
        "def lightweight_segmentation(num_classes=19):\n",
        "    return LightweightSegmentation(ResidualBlock, num_classes)\n",
        "\n",
        "# Evaluation metrics - simplified to be more memory-efficient\n",
        "def compute_metrics(outputs, labels, num_classes=19, ignore_index=255):\n",
        "    \"\"\"\n",
        "    Compute pixel accuracy and mean IoU with minimal memory usage.\n",
        "    \"\"\"\n",
        "    outputs = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Calculate pixel accuracy\n",
        "    mask = (labels != ignore_index)\n",
        "    correct = ((outputs == labels) & mask).sum().float()\n",
        "    total = mask.sum().float()\n",
        "    pixel_acc = correct / (total + 1e-10)\n",
        "\n",
        "    # Calculate IoU for each class - using less memory-intensive approach\n",
        "    iou_sum = 0\n",
        "    iou_count = 0\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        pred_mask = (outputs == cls)\n",
        "        target_mask = (labels == cls)\n",
        "\n",
        "        if target_mask.sum() > 0:  # Only calculate IoU if class exists in ground truth\n",
        "            intersection = (pred_mask & target_mask).sum().float()\n",
        "            union = (pred_mask | target_mask).sum().float()\n",
        "            iou = intersection / (union + 1e-10)\n",
        "            iou_sum += iou\n",
        "            iou_count += 1\n",
        "\n",
        "    miou = iou_sum / (iou_count + 1e-10)\n",
        "    return pixel_acc.item(), miou.item()\n"
      ],
      "metadata": {
        "id": "AG9SVrorVWmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Validation Functions"
      ],
      "metadata": {
        "id": "YGQY_kU-Vb3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# OPTIMIZATION: Simplified training function with memory management\n",
        "def train(model, dataloader, criterion, optimizer, device, scaler=None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Empty CUDA cache before training\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Use gradient accumulation for larger effective batch size\n",
        "    accumulation_steps = 2  # Accumulate gradients for 2 batches\n",
        "\n",
        "    for i, (images, labels) in enumerate(tqdm(dataloader)):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients only at the beginning of accumulation steps\n",
        "        if i % accumulation_steps == 0:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision training block if a scaler is provided\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss = loss / accumulation_steps  # Normalize loss to account for accumulation\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss = loss / accumulation_steps  # Normalize loss\n",
        "            loss.backward()\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        # Clear tensors to free memory\n",
        "        del images, labels, outputs\n",
        "        if i % 5 == 0 and device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "# OPTIMIZATION: Simplified validation function with memory management\n",
        "def validate(model, dataloader, criterion, device, num_classes=19):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    pixel_acc_sum = 0.0\n",
        "    miou_sum = 0.0\n",
        "\n",
        "    # Empty CUDA cache before validation\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute metrics\n",
        "            pixel_acc, mIoU = compute_metrics(outputs, labels, num_classes)\n",
        "            running_loss += loss.item()\n",
        "            pixel_acc_sum += pixel_acc\n",
        "            miou_sum += mIoU\n",
        "\n",
        "            del images, labels, outputs\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    avg_pixel_acc = pixel_acc_sum / len(dataloader)\n",
        "    avg_miou = miou_sum / len(dataloader)\n",
        "    return avg_loss, avg_pixel_acc, avg_miou\n"
      ],
      "metadata": {
        "id": "_PdyNzMXVejc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Training"
      ],
      "metadata": {
        "id": "OoH1M1O4WSm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Hyperparameters - optimized for memory efficiency\n",
        "    batch_size = 8\n",
        "    num_epochs = 25\n",
        "    learning_rate = 0.001\n",
        "    weight_decay = 1e-4\n",
        "    num_classes = 19\n",
        "    ignore_index = 255\n",
        "\n",
        "    # Create datasets and dataloaders with simplified transformations\n",
        "    train_transform = CityscapesTransform(split='train', img_size=(384, 192))\n",
        "    val_transform = CityscapesTransform(split='val', img_size=(384, 192))\n",
        "\n",
        "    train_dataset = CityscapesDataset(subset_path, split='train', transform=train_transform, cache_data=True)\n",
        "    val_dataset = CityscapesDataset(subset_path, split='val', transform=val_transform, cache_data=True)\n",
        "\n",
        "    # Using 2 workers for efficiency\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "    # Create lightweight model and move to device\n",
        "    model = lightweight_segmentation(num_classes).to(device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=learning_rate * 10,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.3\n",
        "    )\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
        "    print(f\"Using mixed precision: {scaler is not None}\")\n",
        "\n",
        "    early_stop_patience = 5\n",
        "    early_stop_counter = 0\n",
        "    best_miou = 0.0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        train_loss = train(model, train_loader, criterion, optimizer, device, scaler)\n",
        "        val_loss, pixel_acc, miou = validate(model, val_loader, criterion, device, num_classes)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Pixel Acc: {pixel_acc:.4f}, mIoU: {miou:.4f}\")\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Current LR: {current_lr:.6f}\")\n",
        "\n",
        "        if miou > best_miou:\n",
        "            best_miou = miou\n",
        "            model_half = model.half()\n",
        "            torch.save(model_half.state_dict(), '/content/drive/MyDrive/best_lightweight_segmentation.pth')\n",
        "            model = model.float()\n",
        "            print(f\"Saved best model with mIoU: {best_miou:.4f}\")\n",
        "            early_stop_counter = 0\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "            print(f\"Early stopping counter: {early_stop_counter}/{early_stop_patience}\")\n",
        "\n",
        "        if early_stop_counter >= early_stop_patience:\n",
        "            print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        if miou >= 0.75:\n",
        "            print(f\"Reached target mIoU of 0.75 at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        scheduler.step()\n",
        "        print()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Training completed in {total_time/60:.2f} minutes!\")\n",
        "    print(f\"Best mIoU: {best_miou:.4f}\")\n",
        "\n",
        "    # Inference demo is in Cell 9 below.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "OrF-YpBnWTTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference Demo and Visualization"
      ],
      "metadata": {
        "id": "T0D_OC1CWWny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_inference_example(model, dataloader, device):\n",
        "    \"\"\"Run inference on a single batch for demonstration\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        images, labels = next(iter(dataloader))\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        start = time.time()\n",
        "        outputs = model(images)\n",
        "        inference_time = time.time() - start\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        print(f\"Inference time for batch of {images.size(0)}: {inference_time:.4f}s\")\n",
        "        print(f\"Per image: {inference_time/images.size(0):.4f}s\")\n",
        "        return images[0], labels[0], preds[0]\n",
        "\n",
        "# Load best model for inference demo\n",
        "try:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = lightweight_segmentation(19).to(device)\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/best_lightweight_segmentation.pth'))\n",
        "    model = model.float()  # Ensure model is in float for inference\n",
        "\n",
        "    print(\"\\nRunning inference demo:\")\n",
        "    # Assume val_loader is defined in the main training (Cell 8) and available here.\n",
        "    # If not, you might need to recreate the validation DataLoader similarly.\n",
        "    _, val_dataset_temp = None, None  # Placeholder if needed; otherwise, reuse the same created dataloader.\n",
        "    # For demo, you may reinstantiate the transform and dataset\n",
        "    val_transform = CityscapesTransform(split='val', img_size=(384, 192))\n",
        "    val_dataset_temp = CityscapesDataset(subset_path, split='val', transform=val_transform, cache_data=True)\n",
        "    val_loader = DataLoader(val_dataset_temp, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "    image, label, pred = run_inference_example(model, val_loader, device)\n",
        "\n",
        "    # Convert tensors for visualization\n",
        "    image_np = image.cpu().numpy().transpose(1, 2, 0)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image_np = std * image_np + mean\n",
        "    image_np = np.clip(image_np, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(131)\n",
        "    plt.imshow(image_np)\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(132)\n",
        "    plt.imshow(label.cpu().numpy(), cmap='nipy_spectral', vmin=0, vmax=18)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(133)\n",
        "    plt.imshow(pred.cpu().numpy(), cmap='nipy_spectral', vmin=0, vmax=18)\n",
        "    plt.title(\"Prediction\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.savefig('/content/drive/MyDrive/segmentation_result.png')\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in inference demo: {e}\")\n"
      ],
      "metadata": {
        "id": "qIR5Gub2WdGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "inference"
      ],
      "metadata": {
        "id": "ZrwMhhjHWiy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2  # OpenCV for image processing\n",
        "\n",
        "# T4 GPU Memory Optimization: Enable benchmark for faster convolutions if input sizes are fixed\n",
        "torch.backends.cudnn.benchmark = True\n"
      ],
      "metadata": {
        "id": "uw_SiXRqWmUK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}