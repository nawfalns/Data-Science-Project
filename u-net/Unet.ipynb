{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# unet training"
      ],
      "metadata": {
        "id": "6QwL1XInv2nb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "libaries"
      ],
      "metadata": {
        "id": "jV0zisWov_rw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQThibja-l0Q",
        "outputId": "1997bb4d-614d-48a2-ef75-109e87d6b08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # metrics visuliztion\n",
        "from PIL import Image # image visulizition\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch # main framework\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from google.colab import drive #dataset from drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Paths to Cityscapes dataset in Google Drive\n",
        "DATASET_PATH = '/content/drive/MyDrive/cityscape'\n",
        "IMG_PATH = os.path.join(DATASET_PATH, 'leftImg8bit') #image set\n",
        "GT_PATH = os.path.join(DATASET_PATH, 'gtFine') # mask set\n",
        "\n",
        "# Define lower resolution for images (to save memory)\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 512\n",
        "\n",
        "# Define the number of classes in Cityscapes (\n",
        "NUM_CLASSES = 19\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "colour map"
      ],
      "metadata": {
        "id": "m0xe4AEowC7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define color map for visualization\n",
        "cityscapes_colors = [\n",
        "    (0, 0, 0),         # 0: unlabeled\n",
        "    (0, 0, 0),         # 1: ego vehicle\n",
        "    (0, 0, 0),         # 2: rectification border\n",
        "    (0, 0, 0),         # 3: out of roi\n",
        "    (0, 0, 0),         # 4: static\n",
        "    (0, 0, 0),         # 5: dynamic\n",
        "    (0, 0, 0),         # 6: ground\n",
        "    (0, 0, 70),        # 7: road - dark blue\n",
        "    (255, 0, 255),     # 8: sidewalk - magenta\n",
        "    (0, 0, 0),         # 9: parking\n",
        "    (0, 0, 0),         # 10: rail track\n",
        "    (255, 165, 0),     # 11: building - orange\n",
        "    (190, 153, 153),   # 12: wall - light brown\n",
        "    (170, 120, 220),   # 13: fence - light purple\n",
        "    (0, 0, 0),         # 14: guard rail\n",
        "    (0, 0, 0),         # 15: bridge\n",
        "    (0, 0, 0),         # 16: tunnel\n",
        "    (153, 153, 153),   # 17: pole - gray\n",
        "    (0, 0, 0),         # 18: polegroup\n",
        "    (250, 170, 30),    # 19: traffic light - amber\n",
        "    (220, 220, 0),     # 20: traffic sign - yellow\n",
        "    (35, 142, 35),     # 21: vegetation - forest green\n",
        "    (152, 251, 152),   # 22: terrain - light green\n",
        "    (70, 130, 180),    # 23: sky - steel blue\n",
        "    (255, 0, 0),       # 24: person - bright red\n",
        "    (255, 127, 0),     # 25: rider - dark orange\n",
        "    (0, 0, 255),       # 26: car - bright blue\n",
        "    (0, 150, 255),     # 27: truck - light blue\n",
        "    (0, 80, 150),      # 28: bus - blue-gray\n",
        "    (0, 0, 110),       # 29: caravan\n",
        "    (0, 0, 110),       # 30: trailer\n",
        "    (0, 80, 100),      # 31: train - dark blue-gray\n",
        "    (0, 80, 100),      # 32: motorcycle - teal\n",
        "    (119, 11, 32),     # 33: bicycle - maroon\n",
        "]\n",
        "# Mapping from Cityscapes IDs to training IDs (ignore label is 255)\n",
        "id_to_trainid = {\n",
        "    0: 255, 1: 255, 2: 255, 3: 255, 4: 255, 5: 255, 6: 255,\n",
        "    7: 0,    # road\n",
        "    8: 1,    # sidewalk\n",
        "    9: 255,  # parking\n",
        "    10: 255, # rail track\n",
        "    11: 2,   # building\n",
        "    12: 3,   # wall\n",
        "    13: 4,   # fence\n",
        "    14: 255, # guard rail\n",
        "    15: 255, # bridge\n",
        "    16: 255, # tunnel\n",
        "    17: 5,   # pole\n",
        "    18: 255, # polegroup\n",
        "    19: 6,   # traffic light\n",
        "    20: 7,   # traffic sign\n",
        "    21: 8,   # vegetation\n",
        "    22: 9,   # terrain\n",
        "    23: 10,  # sky\n",
        "    24: 11,  # person\n",
        "    25: 12,  # rider\n",
        "    26: 13,  # car\n",
        "    27: 14,  # truck\n",
        "    28: 15,  # bus\n",
        "    29: 255, # caravan\n",
        "    30: 255, # trailer\n",
        "    31: 16,  # train\n",
        "    32: 17,  # motorcycle\n",
        "    33: 18,  # bicycle\n",
        "}\n",
        "\n",
        "# Define class names for the 19 classes used for training\n",
        "class_names = [\n",
        "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole',\n",
        "    'traffic light', 'traffic sign', 'vegetation', 'terrain',\n",
        "    'sky', 'person', 'rider', 'car', 'truck', 'bus',\n",
        "    'train', 'motorcycle', 'bicycle'\n",
        "]\n"
      ],
      "metadata": {
        "id": "eKQJF77g_gbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset path"
      ],
      "metadata": {
        "id": "-lEWCdr1SDAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CityscapesSubset(Dataset):\n",
        "    def __init__(self, root, split='train', transforms=None, subset_fraction=0.2):\n",
        "\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.transforms = transforms\n",
        "        self.subset_fraction = subset_fraction\n",
        "\n",
        "        # List cities based on the split\n",
        "        self.cities = os.listdir(os.path.join(IMG_PATH, split))\n",
        "\n",
        "        self.images = []\n",
        "        self.masks = []\n",
        "        for city in self.cities:\n",
        "            img_dir = os.path.join(IMG_PATH, split, city)\n",
        "            mask_dir = os.path.join(GT_PATH, split, city)\n",
        "            for file_name in os.listdir(img_dir):\n",
        "                if file_name.endswith('_leftImg8bit.png'):\n",
        "                    image_id = file_name.replace('_leftImg8bit.png', '')\n",
        "                    mask_name = f\"{image_id}_gtFine_labelIds.png\"\n",
        "                    img_path = os.path.join(img_dir, file_name)\n",
        "                    mask_path = os.path.join(mask_dir, mask_name)\n",
        "                    if os.path.exists(mask_path):\n",
        "                        self.images.append(img_path)\n",
        "                        self.masks.append(mask_path)\n",
        "\n",
        "        # Create a subset of the dataset if needed\n",
        "        if subset_fraction < 1.0:\n",
        "            num_samples = int(len(self.images) * subset_fraction)\n",
        "            indices = []\n",
        "            city_samples = {}\n",
        "            for i, img_path in enumerate(self.images):\n",
        "                city = img_path.split('/')[-2]\n",
        "                city_samples.setdefault(city, []).append(i)\n",
        "            for city, samples in city_samples.items():\n",
        "                city_ratio = len(samples) / len(self.images)\n",
        "                num_city_samples = max(1, int(num_samples * city_ratio))\n",
        "                city_indices = random.sample(samples, min(num_city_samples, len(samples)))\n",
        "                indices.extend(city_indices)\n",
        "            if len(indices) > num_samples:\n",
        "                indices = random.sample(indices, num_samples)\n",
        "            elif len(indices) < num_samples:\n",
        "                remaining = num_samples - len(indices)\n",
        "                all_indices = set(range(len(self.images)))\n",
        "                used_indices = set(indices)\n",
        "                unused_indices = list(all_indices - used_indices)\n",
        "                if unused_indices:\n",
        "                    indices.extend(random.sample(unused_indices, min(remaining, len(unused_indices))))\n",
        "            self.images = [self.images[i] for i in indices]\n",
        "            self.masks = [self.masks[i] for i in indices]\n",
        "\n",
        "        print(f\"Created {split} set with {len(self.images)} images from {len(self.cities)} cities\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.images[idx]\n",
        "        mask_path = self.masks[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        # Resize images\n",
        "        image = image.resize((IMG_WIDTH, IMG_HEIGHT), Image.BILINEAR)\n",
        "        mask = mask.resize((IMG_WIDTH, IMG_HEIGHT), Image.NEAREST)\n",
        "\n",
        "        mask_np = np.array(mask)\n",
        "        mask_out = np.ones_like(mask_np) * 255\n",
        "        for id, train_id in id_to_trainid.items():\n",
        "            mask_out[mask_np == id] = train_id\n",
        "        mask = Image.fromarray(mask_out.astype(np.uint8))\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transforms:\n",
        "            if self.split == 'train':\n",
        "                image, mask = self.transforms(image, mask)\n",
        "            else:\n",
        "                image = TF.to_tensor(image)\n",
        "                mask = torch.from_numpy(np.array(mask)).long()\n",
        "        else:\n",
        "            image = TF.to_tensor(image)\n",
        "            mask = torch.from_numpy(np.array(mask)).long()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "class CityscapesTransforms:\n",
        "    def __init__(self, p_flip=0.5, p_rotate=0.3, p_color=0.5):\n",
        "        self.p_flip = p_flip\n",
        "        self.p_rotate = p_rotate\n",
        "        self.p_color = p_color\n",
        "        self.color_jitter = transforms.ColorJitter(\n",
        "            brightness=0.2,\n",
        "            contrast=0.2,\n",
        "            saturation=0.2,\n",
        "            hue=0.1\n",
        "        )\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "        image = TF.to_tensor(image)\n",
        "        if random.random() < self.p_flip:\n",
        "            image = TF.hflip(image)\n",
        "            mask = TF.hflip(mask)\n",
        "        if random.random() < self.p_rotate:\n",
        "            angle = random.uniform(-10, 10)\n",
        "            image = TF.rotate(image, angle, interpolation=TF.InterpolationMode.BILINEAR)\n",
        "            mask = TF.rotate(mask, angle, interpolation=TF.InterpolationMode.NEAREST)\n",
        "        if random.random() < self.p_color:\n",
        "            image = self.color_jitter(image)\n",
        "        mask = torch.from_numpy(np.array(mask)).long()\n",
        "        return image, mask\n"
      ],
      "metadata": {
        "id": "otaDNDSUDOC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "u-net architecture"
      ],
      "metadata": {
        "id": "ex6_BeRBSKtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# U-Net model components\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Down, self).__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super(Up, self).__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "jT14emdeRbKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "u5dt4HmsSVZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Intersection over Union (IoU)\n",
        "def calculate_iou(pred, target, n_classes):\n",
        "    ious = []\n",
        "    pred = pred.view(-1)\n",
        "    target = target.view(-1)\n",
        "    mask = (target != 255)\n",
        "    pred = pred[mask]\n",
        "    target = target[mask]\n",
        "    for cls in range(n_classes):\n",
        "        pred_inds = pred == cls\n",
        "        target_inds = target == cls\n",
        "        intersection = (pred_inds & target_inds).sum().item()\n",
        "        union = (pred_inds | target_inds).sum().item()\n",
        "        if union == 0:\n",
        "            ious.append(float('nan'))\n",
        "        else:\n",
        "            ious.append(intersection / union)\n",
        "    return ious\n",
        "\n",
        "# Calculate accuracy metrics\n",
        "def calculate_accuracy_metrics(pred, target, n_classes):\n",
        "    mask = (target != 255)\n",
        "    pred = pred[mask]\n",
        "    target = target[mask]\n",
        "    correct = (pred == target).sum().item()\n",
        "    total = mask.sum().item()\n",
        "    pixel_acc = correct / total if total > 0 else 0\n",
        "    class_accuracies = []\n",
        "    for cls in range(n_classes):\n",
        "        target_cls = target == cls\n",
        "        if target_cls.sum().item() > 0:\n",
        "            pred_cls = pred == cls\n",
        "            class_correct = (pred_cls & target_cls).sum().item()\n",
        "            class_total = target_cls.sum().item()\n",
        "            class_accuracies.append(class_correct / class_total)\n",
        "        else:\n",
        "            class_accuracies.append(float('nan'))\n",
        "    valid_accs = [acc for acc in class_accuracies if not np.isnan(acc)]\n",
        "    mean_acc = np.mean(valid_accs) if valid_accs else 0\n",
        "    return pixel_acc, mean_acc, class_accuracies\n",
        "\n",
        "# Visualize predictions\n",
        "def visualize_prediction(image, pred, target, class_colors):\n",
        "    image = image.cpu().permute(1, 2, 0).numpy()\n",
        "    pred = pred.cpu().numpy()\n",
        "    target = target.cpu().numpy()\n",
        "    image = (image - image.min()) / (image.max() - image.min())\n",
        "    pred_color = np.zeros((pred.shape[0], pred.shape[1], 3), dtype=np.uint8)\n",
        "    target_color = np.zeros((target.shape[0], target.shape[1], 3), dtype=np.uint8)\n",
        "    for i, color in enumerate(class_colors[:NUM_CLASSES]):\n",
        "        pred_color[pred == i] = color\n",
        "        target_color[target == i] = color\n",
        "    target_color[target == 255] = (0, 0, 0)\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axs[0].imshow(image)\n",
        "    axs[0].set_title('Input Image')\n",
        "    axs[0].axis('off')\n",
        "    axs[1].imshow(pred_color)\n",
        "    axs[1].set_title('Prediction')\n",
        "    axs[1].axis('off')\n",
        "    axs[2].imshow(target_color)\n",
        "    axs[2].set_title('Ground Truth')\n",
        "    axs[2].axis('off')\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, scheduler=None):\n",
        "    best_miou = 0.0\n",
        "    best_accuracy = 0.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    miou_scores = []\n",
        "    pixel_acc_scores = []\n",
        "    class_acc_scores = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, masks in tqdm(train_loader, desc='Training'):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        iou_scores = []\n",
        "        pixel_accuracies = []\n",
        "        mean_accuracies = []\n",
        "        class_accuracies = [[] for _ in range(NUM_CLASSES)]\n",
        "        with torch.no_grad():\n",
        "            for images, masks in tqdm(val_loader, desc='Validation'):\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                batch_ious = calculate_iou(preds, masks, NUM_CLASSES)\n",
        "                batch_pixel_acc, batch_mean_acc, batch_class_accs = calculate_accuracy_metrics(preds, masks, NUM_CLASSES)\n",
        "                valid_ious = [iou for iou in batch_ious if not np.isnan(iou)]\n",
        "                if valid_ious:\n",
        "                    iou_scores.append(np.mean(valid_ious))\n",
        "                pixel_accuracies.append(batch_pixel_acc)\n",
        "                mean_accuracies.append(batch_mean_acc)\n",
        "                for cls in range(NUM_CLASSES):\n",
        "                    if not np.isnan(batch_class_accs[cls]):\n",
        "                        class_accuracies[cls].append(batch_class_accs[cls])\n",
        "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        mean_iou = np.mean(iou_scores)\n",
        "        miou_scores.append(mean_iou)\n",
        "        mean_pixel_acc = np.mean(pixel_accuracies)\n",
        "        mean_class_acc = np.mean(mean_accuracies)\n",
        "        pixel_acc_scores.append(mean_pixel_acc)\n",
        "        class_acc_scores.append(mean_class_acc)\n",
        "        avg_class_accuracies = []\n",
        "        for cls in range(NUM_CLASSES):\n",
        "            if class_accuracies[cls]:\n",
        "                avg_class_accuracies.append(np.mean(class_accuracies[cls]))\n",
        "        print(f'Training Loss: {epoch_loss:.4f}')\n",
        "        print(f'Validation Loss: {epoch_val_loss:.4f}')\n",
        "        print(f'Mean IoU: {mean_iou:.4f}')\n",
        "        print(f'Overall Pixel Accuracy: {mean_pixel_acc:.4f}')\n",
        "        print(f'Mean Class Accuracy: {mean_class_acc:.4f}')\n",
        "        print(\"\\nPer-class accuracies for important classes:\")\n",
        "        important_classes = [0, 11, 13, 24, 26]  # Example: road, building, car, person, etc.\n",
        "        for cls in important_classes:\n",
        "            if cls < len(class_names) and avg_class_accuracies[cls]:\n",
        "                print(f\"{class_names[cls]}: {avg_class_accuracies[cls]:.4f}\")\n",
        "        if scheduler:\n",
        "            if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                scheduler.step(mean_iou)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "        if mean_iou > best_miou:\n",
        "            best_miou = mean_iou\n",
        "            torch.save(model.state_dict(), 'best_unet_model_iou.pth')\n",
        "            print('Best IoU model saved!')\n",
        "        if mean_pixel_acc > best_accuracy:\n",
        "            best_accuracy = mean_pixel_acc\n",
        "            torch.save(model.state_dict(), 'best_unet_model_accuracy.pth')\n",
        "            print('Best Accuracy model saved!')\n",
        "        print()\n",
        "    print(\"Loading best accuracy model for evaluation...\")\n",
        "    model.load_state_dict(torch.load('best_unet_model_accuracy.pth'))\n",
        "    return model, train_losses, val_losses, miou_scores, pixel_acc_scores, class_acc_scores\n"
      ],
      "metadata": {
        "id": "1-r_6z8TRfvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "setups and data tranformation"
      ],
      "metadata": {
        "id": "GtICOhoASxLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    # Create datasets with transformations\n",
        "    train_transforms = CityscapesTransforms(p_flip=0.5, p_rotate=0.3, p_color=0.8)\n",
        "    train_dataset = CityscapesSubset(\n",
        "        root=DATASET_PATH,\n",
        "        split='train',\n",
        "        transforms=train_transforms,\n",
        "        subset_fraction=0.2  # Use 20% of training data\n",
        "    )\n",
        "    val_dataset = CityscapesSubset(\n",
        "        root=DATASET_PATH,\n",
        "        split='val',\n",
        "        transforms=None,  # No augmentation for validation\n",
        "        subset_fraction=0.3  # Use 30% of validation data\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = UNet(n_channels=3, n_classes=NUM_CLASSES, bilinear=True)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define loss and optimizer (with class weights)\n",
        "    class_weights = torch.ones(NUM_CLASSES).to(device)\n",
        "    class_weights[6] = 2.0   # traffic light\n",
        "    class_weights[11] = 2.0  # person\n",
        "    class_weights[12] = 2.0  # rider\n",
        "    class_weights[17] = 2.0  # motorcycle\n",
        "    class_weights[18] = 2.0  # bicycle\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=255)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model, train_losses, val_losses, miou_scores, pixel_acc_scores, class_acc_scores = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        num_epochs=50,  # Adjust number of epochs as needed\n",
        "        scheduler=scheduler\n",
        "    )\n",
        "\n",
        "    # Plot training progress\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss Curves')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(miou_scores, label='Mean IoU')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean IoU')\n",
        "    plt.legend()\n",
        "    plt.title('IoU Metric')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(pixel_acc_scores, label='Pixel Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Overall Pixel Accuracy')\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(class_acc_scores, label='Mean Class Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Mean Class Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_progress.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Save training metrics\n",
        "    import pandas as pd\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Epoch': range(1, len(train_losses) + 1),\n",
        "        'Training_Loss': train_losses,\n",
        "        'Validation_Loss': val_losses,\n",
        "        'Mean_IoU': miou_scores,\n",
        "        'Pixel_Accuracy': pixel_acc_scores,\n",
        "        'Class_Accuracy': class_acc_scores\n",
        "    })\n",
        "    metrics_df.to_csv('training_metrics.csv', index=False)\n",
        "\n",
        "    # Final evaluation on validation set\n",
        "    model.eval()\n",
        "    all_pixel_accs = []\n",
        "    all_class_accs = []\n",
        "    all_ious = []\n",
        "    print(\"\\nEvaluating final model on validation set...\")\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc='Final Evaluation'):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            batch_ious = calculate_iou(preds, masks, NUM_CLASSES)\n",
        "            batch_pixel_acc, batch_mean_acc, _ = calculate_accuracy_metrics(preds, masks, NUM_CLASSES)\n",
        "            valid_ious = [iou for iou in batch_ious if not np.isnan(iou)]\n",
        "            if valid_ious:\n",
        "                all_ious.append(np.mean(valid_ious))\n",
        "            all_pixel_accs.append(batch_pixel_acc)\n",
        "            all_class_accs.append(batch_mean_acc)\n",
        "    final_miou = np.mean(all_ious)\n",
        "    final_pixel_acc = np.mean(all_pixel_accs)\n",
        "    final_class_acc = np.mean(all_class_accs)\n",
        "    print(f\"\\nFinal Evaluation Results:\")\n",
        "    print(f\"Mean IoU: {final_miou:.4f}\")\n",
        "    print(f\"Overall Pixel Accuracy: {final_pixel_acc:.4f}\")\n",
        "    print(f\"Mean Class Accuracy: {final_class_acc:.4f}\")\n",
        "\n",
        "    # Visualize some validation predictions\n",
        "    test_samples = min(5, len(val_dataset))\n",
        "    for i in range(test_samples):\n",
        "        image, mask = val_dataset[i]\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(image)\n",
        "            pred = torch.argmax(output, dim=1).squeeze(0).cpu()\n",
        "        fig = visualize_prediction(image.squeeze(0), pred, mask, cityscapes_colors)\n",
        "        plt.savefig(f'prediction_{i}.png')\n",
        "        plt.close(fig)\n",
        "\n",
        "    # Create prediction video for a sequence (if available)\n",
        "    try:\n",
        "        sequential_city = val_dataset.cities[0]\n",
        "        city_img_dir = os.path.join(IMG_PATH, 'val', sequential_city)\n",
        "        city_images = sorted([f for f in os.listdir(city_img_dir) if f.endswith('_leftImg8bit.png')])\n",
        "        if len(city_images) > 10:\n",
        "            print(f\"\\nCreating prediction video for {sequential_city}...\")\n",
        "            frames = []\n",
        "            for idx, img_file in enumerate(city_images[:20]):\n",
        "                img_path = os.path.join(city_img_dir, img_file)\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "                image = image.resize((IMG_WIDTH, IMG_HEIGHT), Image.BILINEAR)\n",
        "                image_tensor = TF.to_tensor(image).unsqueeze(0).to(device)\n",
        "                with torch.no_grad():\n",
        "                    output = model(image_tensor)\n",
        "                    pred = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
        "                pred_color = np.zeros((pred.shape[0], pred.shape[1], 3), dtype=np.uint8)\n",
        "                for i, color in enumerate(cityscapes_colors[:NUM_CLASSES]):\n",
        "                    pred_color[pred == i] = color\n",
        "                image_np = np.array(image)\n",
        "                combined = np.hstack([image_np, pred_color])\n",
        "                frames.append(combined)\n",
        "            if frames:\n",
        "                out_path = 'prediction_video.mp4'\n",
        "                height, width, _ = frames[0].shape\n",
        "                fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "                video = cv2.VideoWriter(out_path, fourcc, 5, (width, height))\n",
        "                for frame in frames:\n",
        "                    video.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "                video.release()\n",
        "                print(f\"Prediction video saved to {out_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not create video: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "c30uMT8WRmcJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}