{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# unet training"
      ],
      "metadata": {
        "id": "6QwL1XInv2nb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "libaries"
      ],
      "metadata": {
        "id": "jV0zisWov_rw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQThibja-l0Q",
        "outputId": "1997bb4d-614d-48a2-ef75-109e87d6b08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # metrics visuliztion\n",
        "from PIL import Image # image visulizition\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch # main framework\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from google.colab import drive #dataset from drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Paths to Cityscapes dataset in Google Drive\n",
        "DATASET_PATH = '/content/drive/MyDrive/cityscape'\n",
        "IMG_PATH = os.path.join(DATASET_PATH, 'leftImg8bit') #image set\n",
        "GT_PATH = os.path.join(DATASET_PATH, 'gtFine') # mask set\n",
        "\n",
        "# Define lower resolution for images (to save memory)\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 512\n",
        "\n",
        "# Define the number of classes in Cityscapes (\n",
        "NUM_CLASSES = 19\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "colour map"
      ],
      "metadata": {
        "id": "m0xe4AEowC7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define color map for visualization\n",
        "cityscapes_colors = [\n",
        "    (0, 0, 0),         # 0: unlabeled\n",
        "    (0, 0, 0),         # 1: ego vehicle\n",
        "    (0, 0, 0),         # 2: rectification border\n",
        "    (0, 0, 0),         # 3: out of roi\n",
        "    (0, 0, 0),         # 4: static\n",
        "    (0, 0, 0),         # 5: dynamic\n",
        "    (0, 0, 0),         # 6: ground\n",
        "    (0, 0, 70),        # 7: road - dark blue\n",
        "    (255, 0, 255),     # 8: sidewalk - magenta\n",
        "    (0, 0, 0),         # 9: parking\n",
        "    (0, 0, 0),         # 10: rail track\n",
        "    (255, 165, 0),     # 11: building - orange\n",
        "    (190, 153, 153),   # 12: wall - light brown\n",
        "    (170, 120, 220),   # 13: fence - light purple\n",
        "    (0, 0, 0),         # 14: guard rail\n",
        "    (0, 0, 0),         # 15: bridge\n",
        "    (0, 0, 0),         # 16: tunnel\n",
        "    (153, 153, 153),   # 17: pole - gray\n",
        "    (0, 0, 0),         # 18: polegroup\n",
        "    (250, 170, 30),    # 19: traffic light - amber\n",
        "    (220, 220, 0),     # 20: traffic sign - yellow\n",
        "    (35, 142, 35),     # 21: vegetation - forest green\n",
        "    (152, 251, 152),   # 22: terrain - light green\n",
        "    (70, 130, 180),    # 23: sky - steel blue\n",
        "    (255, 0, 0),       # 24: person - bright red\n",
        "    (255, 127, 0),     # 25: rider - dark orange\n",
        "    (0, 0, 255),       # 26: car - bright blue\n",
        "    (0, 150, 255),     # 27: truck - light blue\n",
        "    (0, 80, 150),      # 28: bus - blue-gray\n",
        "    (0, 0, 110),       # 29: caravan\n",
        "    (0, 0, 110),       # 30: trailer\n",
        "    (0, 80, 100),      # 31: train - dark blue-gray\n",
        "    (0, 80, 100),      # 32: motorcycle - teal\n",
        "    (119, 11, 32),     # 33: bicycle - maroon\n",
        "]\n",
        "# Mapping from Cityscapes IDs to training IDs (ignore label is 255)\n",
        "id_to_trainid = {\n",
        "    0: 255, 1: 255, 2: 255, 3: 255, 4: 255, 5: 255, 6: 255,\n",
        "    7: 0,    # road\n",
        "    8: 1,    # sidewalk\n",
        "    9: 255,  # parking\n",
        "    10: 255, # rail track\n",
        "    11: 2,   # building\n",
        "    12: 3,   # wall\n",
        "    13: 4,   # fence\n",
        "    14: 255, # guard rail\n",
        "    15: 255, # bridge\n",
        "    16: 255, # tunnel\n",
        "    17: 5,   # pole\n",
        "    18: 255, # polegroup\n",
        "    19: 6,   # traffic light\n",
        "    20: 7,   # traffic sign\n",
        "    21: 8,   # vegetation\n",
        "    22: 9,   # terrain\n",
        "    23: 10,  # sky\n",
        "    24: 11,  # person\n",
        "    25: 12,  # rider\n",
        "    26: 13,  # car\n",
        "    27: 14,  # truck\n",
        "    28: 15,  # bus\n",
        "    29: 255, # caravan\n",
        "    30: 255, # trailer\n",
        "    31: 16,  # train\n",
        "    32: 17,  # motorcycle\n",
        "    33: 18,  # bicycle\n",
        "}\n",
        "\n",
        "# Define class names for the 19 classes used for training\n",
        "class_names = [\n",
        "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole',\n",
        "    'traffic light', 'traffic sign', 'vegetation', 'terrain',\n",
        "    'sky', 'person', 'rider', 'car', 'truck', 'bus',\n",
        "    'train', 'motorcycle', 'bicycle'\n",
        "]\n"
      ],
      "metadata": {
        "id": "eKQJF77g_gbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CityscapesSubset(Dataset):\n",
        "    def __init__(self, root, split='train', transforms=None, subset_fraction=0.2):\n",
        "\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.transforms = transforms\n",
        "        self.subset_fraction = subset_fraction\n",
        "\n",
        "        # List cities based on the split\n",
        "        self.cities = os.listdir(os.path.join(IMG_PATH, split))\n",
        "\n",
        "        self.images = []\n",
        "        self.masks = []\n",
        "        for city in self.cities:\n",
        "            img_dir = os.path.join(IMG_PATH, split, city)\n",
        "            mask_dir = os.path.join(GT_PATH, split, city)\n",
        "            for file_name in os.listdir(img_dir):\n",
        "                if file_name.endswith('_leftImg8bit.png'):\n",
        "                    image_id = file_name.replace('_leftImg8bit.png', '')\n",
        "                    mask_name = f\"{image_id}_gtFine_labelIds.png\"\n",
        "                    img_path = os.path.join(img_dir, file_name)\n",
        "                    mask_path = os.path.join(mask_dir, mask_name)\n",
        "                    if os.path.exists(mask_path):\n",
        "                        self.images.append(img_path)\n",
        "                        self.masks.append(mask_path)\n",
        "\n",
        "        # Create a subset of the dataset if needed\n",
        "        if subset_fraction < 1.0:\n",
        "            num_samples = int(len(self.images) * subset_fraction)\n",
        "            indices = []\n",
        "            city_samples = {}\n",
        "            for i, img_path in enumerate(self.images):\n",
        "                city = img_path.split('/')[-2]\n",
        "                city_samples.setdefault(city, []).append(i)\n",
        "            for city, samples in city_samples.items():\n",
        "                city_ratio = len(samples) / len(self.images)\n",
        "                num_city_samples = max(1, int(num_samples * city_ratio))\n",
        "                city_indices = random.sample(samples, min(num_city_samples, len(samples)))\n",
        "                indices.extend(city_indices)\n",
        "            if len(indices) > num_samples:\n",
        "                indices = random.sample(indices, num_samples)\n",
        "            elif len(indices) < num_samples:\n",
        "                remaining = num_samples - len(indices)\n",
        "                all_indices = set(range(len(self.images)))\n",
        "                used_indices = set(indices)\n",
        "                unused_indices = list(all_indices - used_indices)\n",
        "                if unused_indices:\n",
        "                    indices.extend(random.sample(unused_indices, min(remaining, len(unused_indices))))\n",
        "            self.images = [self.images[i] for i in indices]\n",
        "            self.masks = [self.masks[i] for i in indices]\n",
        "\n",
        "        print(f\"Created {split} set with {len(self.images)} images from {len(self.cities)} cities\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.images[idx]\n",
        "        mask_path = self.masks[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        # Resize images\n",
        "        image = image.resize((IMG_WIDTH, IMG_HEIGHT), Image.BILINEAR)\n",
        "        mask = mask.resize((IMG_WIDTH, IMG_HEIGHT), Image.NEAREST)\n",
        "\n",
        "        mask_np = np.array(mask)\n",
        "        mask_out = np.ones_like(mask_np) * 255\n",
        "        for id, train_id in id_to_trainid.items():\n",
        "            mask_out[mask_np == id] = train_id\n",
        "        mask = Image.fromarray(mask_out.astype(np.uint8))\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transforms:\n",
        "            if self.split == 'train':\n",
        "                image, mask = self.transforms(image, mask)\n",
        "            else:\n",
        "                image = TF.to_tensor(image)\n",
        "                mask = torch.from_numpy(np.array(mask)).long()\n",
        "        else:\n",
        "            image = TF.to_tensor(image)\n",
        "            mask = torch.from_numpy(np.array(mask)).long()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "class CityscapesTransforms:\n",
        "    def __init__(self, p_flip=0.5, p_rotate=0.3, p_color=0.5):\n",
        "        self.p_flip = p_flip\n",
        "        self.p_rotate = p_rotate\n",
        "        self.p_color = p_color\n",
        "        self.color_jitter = transforms.ColorJitter(\n",
        "            brightness=0.2,\n",
        "            contrast=0.2,\n",
        "            saturation=0.2,\n",
        "            hue=0.1\n",
        "        )\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "        image = TF.to_tensor(image)\n",
        "        if random.random() < self.p_flip:\n",
        "            image = TF.hflip(image)\n",
        "            mask = TF.hflip(mask)\n",
        "        if random.random() < self.p_rotate:\n",
        "            angle = random.uniform(-10, 10)\n",
        "            image = TF.rotate(image, angle, interpolation=TF.InterpolationMode.BILINEAR)\n",
        "            mask = TF.rotate(mask, angle, interpolation=TF.InterpolationMode.NEAREST)\n",
        "        if random.random() < self.p_color:\n",
        "            image = self.color_jitter(image)\n",
        "        mask = torch.from_numpy(np.array(mask)).long()\n",
        "        return image, mask\n"
      ],
      "metadata": {
        "id": "otaDNDSUDOC2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}